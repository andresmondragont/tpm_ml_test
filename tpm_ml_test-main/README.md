# Team Process Mapping Take-Home Task: [YOUR NAME HERE].

Goal: In this pre-test, you will first read brief selections from two social science papers (Step 1). You will then go through an end-to-end implementation of a feature and apply it to a dataset of team conversations (Step 2). Finally, you will write a reflection on how well you think this feature extractor performed on the data, as well as how well it performs in operationalizing social science constructs (Step 3).

The idea behind this task is to give you a flavor of the scope of our work — to show how we take inspiration from social science, then apply these ideas in a computational way.

Please write your reflection in this README document.

## 1. High-Level Questions
1a. Which dataset did you choose?

> I decided to use the CSOP dataset primarily because of the size of the dataset. Although, I am aware that Apache Spark could also have been helpful to handle the bigger dataset of Juries since it would have allowed me to parallelize the computations. Yet, I decided not to implement this for this task.

1b. What method(s) did you choose? In 1-2 sentences each, describe your sentiment analysis method(s).

> I used the suggested method from HuggingFace, Twitter-roBERTa-base for Sentiment Analysis - UPDATED (2022), because it was the most useful for the objective of classifying to the three different types of sentiment labels (0 → Negative; 1 → Neutral; 2 → Positive). 

Essentially, this method uses a RoBERTa-base model (pretrained model on the English language) and finetuned for sentiment analysis using the TweetEval benchmark (multi-class tweet classification) to label the different chats into the categories Negative, Neutral, and Positive and provide a probability for each of the labels using Softmax.

1c. Does your method capture any of the ideas from Troth et al. and West et al.? If so, which ones?

> In Troth et al., they discuss the relationship between team members’ emotional skills and their performance in teams. This includes being aware of both your own emotions and your teammates’ emotions as well as being able to manage your own and others’ emotions. In this way, particularly with regards to being cognizant of others’ emotions, predictive models can help in the goals of recognizing others’ emotional displays and detecting false emotional expressions, and accurately identifying the emotion of a fellow team member which is useful for effective team communication and team-conflict resolution. The Twitter-roBERTa-base method used was finetuned using TweetEval which consists of datasets including emotion recognition, irony detection, and sentiment analysis. Thus, the method used allows to classify chats between team members into different emotions, which can be used as an indicator of how a teammate feels which in turn can impact the team's performance. Regarding West et al., there is a focus on Positive Organizational Behavior (POB) which highlights team efficacy, optimism, and resilience as main capacities of interest. These variables exist at the team level (in the CSOP case this would be the chats exchanged between people in the team) and can be used to determine the degree of cohesion, conflict, satisfaction, coordination and cooperation in teams. All of these areas can be measured by the positivity in the conversations between teammates, which the Twitter-roBERTa-base method can identify. Usually the higher the POB capacity (reflected by a higher positivity expressed by a chat), the better the team outcomes.

1d. Compared to how Troth et al. and West et al. measured positivity, what are some strengths and weaknesses of your approach?

> On the one hand, Troth et al. measure positivity by having each respondent provide peer ratings of each of their fellow team members in terms of their communication performance in the team. On the other hand, West et al. measured positivity in their questionnaire as an assessment of team capacity by making the target of each item the team rather than the individual. Thus, some strengths of the model are: the emotion of a chat between team members can be classified using 3 distinct labels (Negative, Neutral, Positive); the model has been recently updated and has been trained on Twitter data from 2018-2021 which is recent and reflects the way in which people currently communicate about tasks; and the model allows for nuances in the sentiment of a chat by providing the probabilities of different emotions, which is useful because rarely is a comment expressive of only one emotion. Some drawbacks to our approach, in relation to the literature, are: we classify the chats made by someone while the literature focuses on having others evaluate their overall team or other team members; the procedure of Troth et al. made teammates be rated by fellow team members in two different surveys, which can allow for averages to be made; and West et al. records ratings for a team using a 6-point Likert scale, which can provide a better estimate of a construct for a team than a sentiment classification like in our approach.

## 2. Method evaluation
Next, we would like you to consider how you would evaluate your method. How do you know the classification or quantification of emotion is “right?” Try to think critically!

2a. Open up your output CSV and look at the columns you generated. Do the values “make sense” intuitively? Why or why not?

> Looking at a few rows from the sentiment column, most of the sentiment classifications make sense. For instance, “sounds good” has a probability of 0.796 for the “Positive” label while the message “shoot that messed it up. dragged the wrong one” has a probability of 0.864 for “Negative”.
However, there are some messages like “106” that are just numbers (given the nature of other messages, the task given to respondents involved talking about figures so this is expected). In the case of “106”, this has a probability of 0.569 for “Neutral” and a probability of 0.341 for “Positive”. While it makes sense that the highest probability belongs to “Neutral”, the “Positive” probability is higher than I would expect, particularly because it is just a number.

2b. Propose an evaluation mechanism for your method(s). What metric would you use (e.g., F1, AUC, Accuracy, Precision, Recall)?

> I would focus on the Misclassification Rate, which is just 1 - Accuracy.

2c. Describe the steps you would take in evaluating this method. Be as specific as possible.

> First, I would have to make sure a specific label is attached to each of the instances so I would create a label column that contains the value of one of the three emotions that has the highest probability. Then, I would train the text classification model using this updated labeled dataset. To go about training the pre-trained model I could use fine-tuning and try to modify the model to best fit the labeled dataset. After training, I could then evaluate the performance of the model on a test set. In this way, I will be able to use your trained model to predict the labels of unseen data and compare the predicted labels with the true labels to then calculate the Misclassification Rate.

2d. Given the nature of these datasets, what challenges do you anticipate that you may encounter during evaluation? How would you go about resolving them?

> With text data, there is always the risk of having people misspell words and using abbreviations. This can make the training data unique in the sense that different variations of a word or phrase may be used in the training data and the model may overfit the very specific words written, which can then lead to overfitting and make the model perform poorly on the test dataset. A way to deal with this, given a similar task I have worked on in my previous research position, would be to create/obtain a dictionary of spelling variations of common words (e.g. ‘omg’ and ‘oh my God’) which could be implemented in the preprocessing stage of cleaning the data.

## 3. Overall reflection
3a. How much time did it take you to complete this task? (Please be honest; we are looking for feedback to make sure the task is scoped appropriately, as this is one of the first times we’re using this task.)

> It took me ~3 days to complete. The part that took the longest was trying to make sense of using Python in an IDE environment instead of Google Colab or Anaconda. Other than that, the readings were very reasonable, and understanding the HuggingFace sentiment analysis method used also took a reasonable amount of time.

3b. Finally, provide an overall reflection of your experience. How did you approach this task? What challenge(s) did you encounter? If you had more time, what are additional extensions, improvements, or tests that you would want to implement?

> First of all, running the code using a Python IDE (IDLE and VSCode) challenged me by giving me several errors including not finding installed packages because I had to use the Python environment from base to conda; having errors where the imports would not recognize the file names (importing Python functions from other scrips was giving me errors in calculate_chat_level_features.py and in calculate_conversation_level_features.py); getting an error that the code could not find a specific column in the dataset (this column did exist); and the code taking too long to run. For this reason, I decided to implement the get_sentiment() function on the sentiment_features.py file and save it, yet, since running the code using an IDE was giving me issues (which I was not able to solve from Googling) I decided to use an environment I am familiar with using which is Google Colab. I decided to write my function for sentiment analysis using the Twitter-roBERTa-base model and run this first on a few simple texts like “so horrible” and “i love that” just to have a sanity check that the model was working. Once I noticed it was working the way I expected it to, I decided to apply the function to the column “message” from the file csopII_output_chat_level.csv (which is the output of running the script on the file once). The inevitable several attempts at running it were taking too long to the point that beyond the 2 hours mark of running my Colab would stop running. So, I decided to run the function on a random message, which took ~3 seconds to complete. I noticed that given the size of the dataset (5k rows) it would take around 4 hours to run. In order to make the best use of time, I decided to take a random sample of 500 rows from the dataset and use that to run my function. This ended up working in a reasonable amount of time which allowed me to complete the entire task and still get reasonable results.
Overall, if I had more time, I would first prioritize getting help regarding the errors I kept getting from running the script using VSCode and IDLE. Second, I would focus on running the function on the entire dataset, perhaps by using parallelization methods that would decrease the amount of time it takes to run. I would also like to explore other text classification models perhaps from sources other than HuggingFace so that I can check the accuracy of the probabilities for the different labels. It would be interesting to see the averages for different classification probabilities or even use a model that is based on TweetEval and its Offensive Language Identification which shows how offensive a message is, which can be useful for the purposes of assessing how teams communicate in team activities.
